{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some kind of scrape that will collect a broad selection of EXIF fields to create a master var list.\n",
    "for each ID, there will be a fromkeys dict created, then we will cycle through the incoming exif data to fill in the columns according to available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flickr Librarian\n",
    "\n",
    "- saved txt file of the latest page scraped (offset)\n",
    "offset file has 3 lines:\n",
    "-- highest page number scanned (OFFSET)\n",
    "-- flickr id of the newest known image (NNI)\n",
    "-- flickr id of the oldest known image (ONI)\n",
    "\n",
    "\n",
    "\n",
    "temporary offset = 0\n",
    "\n",
    "- query flickr for page 1\n",
    "    - save initial flicker ID, will be NNI at end of session\n",
    "\n",
    "- per page query loop:\n",
    "    - get all ids (known process)\n",
    "    - is NNI in the list? get index\n",
    "    - get exif for all photo ids before NNI.\n",
    "        - save exif to dataframe, tbd how often these will be dumped to SQL\n",
    "    - temp offset += 1\n",
    "\n",
    "if NNI not on that page, do next page.\n",
    "Continue until NNI is found and its full page is scraped.\n",
    "\n",
    "go to page OFFSET + TEMPORARY OFFSET - 1\n",
    "\n",
    "- scan page for ONI\n",
    "- scan page +1 for ONI\n",
    "- scan page +2 for ONI\n",
    "\n",
    "If ONI not found: break\n",
    "\n",
    "if ONI found:\n",
    "- scrape IDs after ONI\n",
    "- Go to next page\n",
    "- scrape all IDs\n",
    "- repeat until flickr cuts us off.\n",
    "- Or have a built in limit that dumps to SQL/updates offset file every 100 new records\n",
    "\n",
    "\n",
    "END OF SESSION\n",
    "- dump dataframe to SQL\n",
    "- Update ONI\n",
    "- Update NNI\n",
    "- update OFFSET\n",
    "- save to offset file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIDDLE STEP\n",
    "\n",
    "- just make sure the EXIF data includes user comments\n",
    "- d41d8cd98f00b204e9800998ecf8427e == zeta stitch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
